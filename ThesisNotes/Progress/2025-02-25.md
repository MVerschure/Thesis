Last week I had a (short) meeting with Martijn. Here is a short list of things discussed:
- [ ] Asking help online is super cool!!
- [ ] Hand calculation of how an update would look
	- This would depend on the type of inference used
- [ ] Write something simple in python that might work.
#### Today's Goal
Exactly the list shown above, starting with the calculations by hand. This will possibly provide a framework for what to program in python. 
#### Calculation By Hand
Say I would want to use the message passing algorithm to perform the inferencing, I would need the probabilistic model of the situation. [This paper](https://www.mdpi.com/1099-4300/25/8/1138) provides an explanation and example of how to perform the message passing algorithm in using a mixture node. 

##### A Given Example
Let me first try to run through their example before messing things up :). The figure below is the visualization of their probabilistic model using a FFG (Forney-style factor graph). Essentially, it comprises of of 2 probabilistic models $p(y,s|m_1=1)$ and $p(y,s|m_2=1)$, which are "selected" by the one-hot encoded variable $m=[m_1, m_2]$ resulting in a mixture model $s$. The individual probabilistic models can be written as 
$$
\tag{1}
\begin{align}
p(y, s|m_1=1) = p(y|s)p(s|m_1=1)\\
p(y, s|m_2=1) = p(y|s)p(s|m_2=1)\\
\end{align}
$$
Which share the same likelihood model. The model selection variable is subject to the prior
$$
\tag{2}
p(m) = Ber(m|\pi) = \begin{cases}
	p(m_1=1) = \pi\\
	p(m_2=1) = (1-\pi)
\end{cases}
$$
The complete mixture model can than be specified by
$$
\tag{3}
p(y, s, m) = p(m)p(y|s)\prod_{k=1}^{2}p(s|m_k=1)
$$
![[entropy-25-01138-g004.webp | center | 400]]
Just going to type along here:
Suppose we are interested in computing the posterior probabilities $p(s|y=\hat{y})$, marginalized over the distinct models, and $p(m|y=\hat{y})$. The model evidence ($p(y=\hat{y})$) of both models can be computed using **scale factors locally on the edge corresponding to $s$ as**
$$
\tag{4}
\begin{align}
	p(y=\hat{y}|m_1=1)= \int \overrightarrow{\mu}_{s|m_1=1}(s)\overleftarrow{\mu}_s(s)ds = \int p(s|m_1=1)p(y=\hat{y}|s) ds \\
	p(y=\hat{y}|m_2=1)= \int \overrightarrow{\mu}_{s|m_2=1}(s)\overleftarrow{\mu}_s(s)ds = \int p(s|m_2=1)p(y=\hat{y}|s) ds 
\end{align}
$$
The important claim to understand what is going on is explained in theorem 1:
	**Theorem 1.** *Consider an acyclic FFG $\mathcal{G} = (\mathcal{V}, \mathcal{E})$. The model evidence of the corresponding model $p(y=\hat{y}, s)$ can be computed at any edge in the graph as $\int\overrightarrow{\mu}_{s_j}(s_j)\overleftarrow{\mu}_{s_j}(s_j)ds_j$ for all $j\in\mathcal{E}$  and at any node ... \[an alternative formulation for nodes\]*
*However*, equation $4$ has more going on than just calculating the posterior. Clearly the value for $p(y=\hat{y}|m_k=1)$ is calculated instead of the mentioned $p(y=\hat{y})$ (assuming the $s$ can be overlooked). The figure below might be able to give some more insight. The model evidence is calculated as if one of the 2 models is active.
![[entropy-25-01138-i001.png | center | 500]]
Some message definitions might help gain some insight.

| Message                                 | Funcitonal Form                                                                              | Interpretation                                                                                                                                                                              |
| --------------------------------------- | -------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| $\overleftarrow{\mu}_m(m)$              | $\prod_{k=1}^K\int \overrightarrow{\mu}_{s_j\|m_1=1}(s_j)\overleftarrow{\mu}_{s_j}(s_j)ds_j$ | the unnormalized distribution over<br>the model evidence corresponding to the individual models. Based on the scale factors of<br>the incoming messages, the model evidence can be computed |
| $\overrightarrow{\mu}_{s_j}(s_j)$       | $\sum_{k=1}^K\overrightarrow{\mu}_m(m_k=1)\overrightarrow{\mu}_{s_j\|m_k=1}(s_j)$            | a mixture distribution over the incoming messages ... (there seems to be an mistake in the paper), with the weight determined, in part, by $\overrightarrow{\mu}_m(m)$                      |
| $\overleftarrow{/mu}_{s_j\|m_k=1}(s_j)$ | $\overleftarrow{\mu}_{s_j}(s_j)$                                                             |                                                                                                                                                                                             |
 From these message definitions, we can see that the calculation of equation $4$ takes place inside the mixture node for computing  $\overleftarrow{\mu}_m(m)$. Together with the forward message over edge $m$, we obtain the posterior.
$$
\tag{5}
p(m|y=\hat{y}) = \frac{\overrightarrow{\mu}_m(m)\overleftarrow{\mu}_m(m)}{\sum_{k=1}^2 \overrightarrow{\mu}_m(m_k=1) \overleftarrow{\mu}_m(m_k=1)} = \frac{p(m)p(y=\hat{y}|m)}{p(y=\hat{y})}
$$
This equation makes use of a more general definition within the message passing algorithm for calculating posteriors (*equation $6$ in the paper*). The example relates this back further to a possible calculation.
##### Relate it Back
We've just looked at a simple, yet far from trivial case of how the message passing algorithm might work for a mixture node. Now is the time to start relating it back to what I would want to achieve. It is time to introduce the factorizations and do some calculations. 

I might be biased, but I am convinced the model I would want shares some important similarities with the previously discussed example, but lets not get ahead of myself. Lets consider the case where I have 2 object "slots" and each object only has a one dimensional property, namely the position. Then I have a single observation $y$ with a relation to only one of the 2 objects. In the simple case, one object, I could define a probabilistic model as
$$
\tag{6}
\begin{align}
	p(y,s) &= p(y|s)p(s)\\
	p(s) &= \mathcal{N}(x, v)\\
	p(y|s) &= \mathcal{N}(s, w) 
\end{align}
$$
Here, the latent variable $s$ represents the believe about the position of the single object, modeled by a normal curve with mean $x$ and variance $v$. Should inference be successful, $x$ will approach the actual position of the object with $v \rightarrow 0$. Given this believe and some uncertainty in our measurement captured in $w$, we expect our measurement to also be a normal distribution, around our believe and with this uncertainty. 

Now, with our 2 object system there would be 2 probabilistic models
$$
\tag{7}
\begin{align}
p(y, s | m_1=1) = p(y|s)p(s|m_1=1)\\
p(y, s | m_2=1) = p(y|s)p(s|m_2=1)
\end{align}
$$
Which is the same as what is described in equation $1$. The likelihood model remains the same as that in equation $6$: $p(y|s) = \mathcal{N}(s, w)$, and the object specification lies within the definition $p(s|m_k=1)$ (*There is some room for generalization here :*). For if we have 2 object with believes about positions $x_1$ and $x_2$ we can formulate the following. 
$$
\tag{8}
\begin{align}
p(s|m_1=1) = \mathcal{N}(x_1, v_1)\\
p(s|m_2=1) = \mathcal{N}(x_2, v_2)
\end{align}
$$
Some difficulty arises in the definition of $p(m)$. Intuitively, $p(m_k=1)$ should represent the chance that the observation $y$ originates from the object represented by $p(s|m_k=1)$. For the sake of simplicity in a test calculation, we can assume a certain *categorical* distribution for testing purposes. **Sanity check:** in the the extreme cases, i.e. $p(m) = Cat([1, 0]) \vee Cat([0, 1])$, the inferencing process should converge to the inferencing of a single object. 

Now all that is left to do is to set a prior believe about some objects and "take" a measurement :). Lets assume the following
$$
\tag{9}
\begin{align}
p(s|m_1&=1) = \mathcal{N}(0, 1)\\
p(s|m_2&=1) = \mathcal{N}(10, 2)\\
\hat{y} &= 1 \textrm{ with } w=0.5\\
p(m) &= Cat([0.8, 0.2])
\end{align}
$$
##### Casual Bayesian Inference
Since this models is super simple, computation of the exact model evidence term is possible, thus performing inference simply by using Bayes rule should be possible. But this is a task for tomorrow :)