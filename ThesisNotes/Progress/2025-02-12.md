Since the meeting with Martijn I've been contemplating the construction of some "correlation" matrix $R$. For an element $R_{ij}$, $i$ would correspond to an object within the believe about the world and $j$ would be referring to an object in the observation. It may be obvious that this method uses a object-centered world description.
#### What's Next?
So in my mind, two correlated questions are to be answered:
- How to (correclty) use $R$?
- How to construct $R$?
These questions are correlated for the way $R$ is used determines the "units" of the elements of $R$.

To figure out how to use $R$, it is essential to know how an update would be performed for a single object. For essentially, the usage of $R$ reduces the update of all objects, to an update of a single object with a "mixed" observation. I envision something like:
$$
Y_q = RY_o
$$
Where $Y_o$ represents a "vector" of individual observed objects and $Y_q$ should represent a new vector where each element represent a "mixture" of distributions found in $Y_o$. For $Y_q$ to be a proper mixture, **rows of $R$ need to be normalized**. 
#### One at a Time
Lets run with this idea of constructing a `MixtureDistribution` for the observation $Y_{q, i}$ to update our believe about object $i$. How would a model for one object look like? Previously, for inference of a one dimensional position about one object we had:
```julia
@model function one_obj_obs(_u, _v, y)
	x ~ Normal(mean=_u, variance=_v)
	y ~ Normal(mean=x, variance=0.5)
end

# The autoupdates specify we want to update
# when a new observation y arrives.
autoupdates = @autoupdates begin
	_u = mean(q(x))
	_v = var(q(x))
end
```
**Important to note** here is that the inferencing about `x` is made possible given the relation between `x` and `y`: `x = mean(q(y))`. This relation still holds, but might be harder to explicitly formulate in the model.  Now, `y` would contain a list of measured values about more objects in the scene and a new piece of information needs to be included in the observations: the row $R_i$. A most definitely not working model would sortof look like this. 
```julia
@model function multi_obj_obs(_u, _v, Ri, y)
	# The actual state x is still a normal dist
	x ~ Normal(mean=_u, variance=_v)
	
	# Now the "observation" needs to be constructed
	for i in length(y)
		o[i] ~ Normal(mean=y[i], variance=0.5)

	obs ~ Mixture(switch=Ri, inputs = o)
end

autoupdates = @autoupdates begin
	_u = mean(q(x))
	_v = var(q(x))
end
```
As can be seen, the relation between `x` and `y` that was explicit in the `one_obj_obs` model is not persistent in the `multi_obj_obs` model:  At most one of the normal distributions used in constructing the mixture model is due to our object described by `x`.

For future reference it might be good to note that this method should be applied on a per object base. An inference engine should be made separately for each object. As a result, $R$ should be determined outside of the model. After all, it is passed to the model as a variable. A possible solution to it all might be to formulate a model in which all objects included within a single model and the calculation of $R$ should be performed in the model.
#### A Change of Pace
Time for the second question. How to construct $R$. Clearly, an element $R_{ij}$ should represent some form of similarity between an object in the believe $i$ and the observation $j$. Intuitively, any measure of similarity can be used as long as a normalization procedure is applied afterwards. The tricky part is that the element $R_{ij}$ is taken to be the chance that observation $j$ belongs to object $i$, this while the similarity between these elements is not by definition equal to the probability of these elements referring to the same real-world object. Until I've found a mores rigorous way of calculating this probability, using the **Kullback-Leibler** divergence as a measure of similarity might be fruit full.

Statistically, for 2 distributions $Q$ and $Y$, this divergence represents the expected value for the diffence in log-probabilities: $KL[Q, Y] = E\{log[Q(x)] - log[Y(x)]\}$.  