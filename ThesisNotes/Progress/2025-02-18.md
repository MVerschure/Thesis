The last few days I've spent considerable time trying to work mixture models. Oftentimes the things I tried did not work as expected. Though the creation of static mixture densities is fairly straight forward, the inferencing part often seems to mess things up. Especially in the "online" case. I'm done staring at errors and would like to feel moving forward again.
#### Forward.
I observed an interesting workflow while working through some of the examples given for inference on *Gaussian Mixture* models. They start by specifying a probabilistic model $p(x, y) = f(x, y)$ before implementing a solution. Might be a good thing to think about applying a similar strategy. 
#### Defining $P(\cdot)$ 
First lets sketch the setting I want to model. Two objects exist about which observations are being made. Each object only has a 1D position as attribute. In the "final" implementations, a single observation will contain a list of detected objects, so intuitively the model would need to be able to handle a vector of observations. An alternative approach would be to handle each object as an individual observation. At the very least this would make the model more flexible to a varying number of detected object. 

I would propose the following structure:
![[Screenshot from 2025-02-18 15-50-21.png]]
Here $o_i$ represents an object, $y$ the observation and $s$ is a switch which determines how the objects relate to the observation. In essence, $s_i$ should contain the probability that observation $y$ originates from object $o_i$, for each $i$. The contents of $s$ can then be determined as mentioned in [[2025-02-12]]. This would lead to the following structure:
![[Screenshot from 2025-02-18 16-03-24.png]]
This does introduce a loop, but it might be worth a shot. 

For future reference, this approach is inspired by [this paper](https://www.mdpi.com/1099-4300/25/8/1138). 
##### More Precise Definition

